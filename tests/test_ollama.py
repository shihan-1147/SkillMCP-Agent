"""
Ollama è¿æ¥æµ‹è¯•

æµ‹è¯•æœ¬åœ° Ollama æœåŠ¡æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import asyncio
import sys
from pathlib import Path

# ç¡®ä¿è·¯å¾„æ­£ç¡®
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


async def test_ollama():
    """æµ‹è¯• Ollama æœåŠ¡"""
    print("=" * 60)
    print("ğŸ§ª Testing Ollama Connection")
    print("=" * 60)
    
    # 1. æµ‹è¯•é…ç½®
    print("\nğŸ“‹ Test 1: Configuration")
    from src.core.config import get_settings
    settings = get_settings()
    print(f"  âœ… LLM Provider: {settings.llm_provider}")
    print(f"  âœ… Ollama Base URL: {settings.ollama_base_url}")
    print(f"  âœ… Ollama Model: {settings.ollama_model}")
    print(f"  âœ… Embedding Provider: {settings.rag_embedder_type}")
    print(f"  âœ… Embedding Model: {settings.ollama_embedding_model}")
    print(f"  âœ… Embedding Dimension: {settings.embedding_dimension}")
    
    # 2. æµ‹è¯• Ollama å®¢æˆ·ç«¯
    print("\nğŸ“‹ Test 2: Ollama LLM Client")
    from src.core.ollama import get_ollama_client
    
    client = get_ollama_client()
    
    # æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
    try:
        models = await client.list_models()
        print(f"  âœ… Available models: {len(models)}")
        for model in models[:5]:
            print(f"     - {model}")
        if len(models) > 5:
            print(f"     ... and {len(models) - 5} more")
    except Exception as e:
        print(f"  âŒ Failed to list models: {e}")
        print("  âš ï¸ è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")
        return False
    
    # æ£€æŸ¥ç›®æ ‡æ¨¡å‹
    model_available = await client.check_model(settings.ollama_model)
    if model_available:
        print(f"  âœ… Model '{settings.ollama_model}' is available")
    else:
        print(f"  âš ï¸ Model '{settings.ollama_model}' not found")
        print(f"     Run: ollama pull {settings.ollama_model}")
    
    # 3. æµ‹è¯• LLM å¯¹è¯
    print("\nğŸ“‹ Test 3: LLM Chat")
    try:
        response = await client.chat(
            messages=[
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"}
            ],
            temperature=0.7,
            max_tokens=100
        )
        
        if response["success"]:
            print(f"  âœ… LLM Response: {response['content'][:100]}...")
        else:
            print(f"  âŒ LLM Error: {response.get('error')}")
    except Exception as e:
        print(f"  âŒ LLM Chat failed: {e}")
    
    # 4. æµ‹è¯• Embedding
    print("\nğŸ“‹ Test 4: Ollama Embedding")
    from src.rag.embedder import get_embedder, set_embedder
    
    # é‡ç½® embedder
    set_embedder(None)
    
    try:
        embedder = get_embedder("ollama")
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬
        text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯ Ollama Embedding åŠŸèƒ½ã€‚"
        embedding = await embedder.embed_text(text)
        
        print(f"  âœ… Embedding dimension: {len(embedding)}")
        print(f"  âœ… First 5 values: {embedding[:5]}")
        
        # æµ‹è¯•æ‰¹é‡æ–‡æœ¬
        texts = [
            "ä»€ä¹ˆæ˜¯ Agentï¼Ÿ",
            "MCP åè®®æ˜¯ä»€ä¹ˆï¼Ÿ",
            "RAG å¦‚ä½•å·¥ä½œï¼Ÿ"
        ]
        embeddings = await embedder.embed_texts(texts)
        print(f"  âœ… Batch embedding: {len(embeddings)} texts embedded")
        
    except Exception as e:
        print(f"  âŒ Embedding failed: {e}")
        print("  âš ï¸ è¯·ç¡®ä¿å·²æ‹‰å– embedding æ¨¡å‹: ollama pull qwen3-embedding:latest")
    
    # 5. æµ‹è¯• RAG Pipeline
    print("\nğŸ“‹ Test 5: RAG Pipeline with Ollama")
    try:
        from src.rag import get_rag_pipeline
        from src.rag.embedder import OllamaEmbedder
        
        # é‡ç½® embedder
        set_embedder(None)
        
        rag = get_rag_pipeline()
        
        # åŠ è½½æµ‹è¯•æ–‡æ¡£
        docs_dir = Path(settings.documents_dir)
        if docs_dir.exists():
            await rag.load_documents(docs_dir)
            print(f"  âœ… Loaded documents: {rag.stats.get('total_chunks', 0)} chunks")
            
            # æµ‹è¯•æ£€ç´¢
            results = await rag.retrieve("ä»€ä¹ˆæ˜¯ Agent", top_k=2)
            print(f"  âœ… Retrieved {len(results)} results")
            
            for i, r in enumerate(results[:2], 1):
                print(f"     {i}. Score: {r.score:.4f} - {r.chunk.content[:50]}...")
        else:
            print(f"  âš ï¸ Documents directory not found: {docs_dir}")
            
    except Exception as e:
        print(f"  âŒ RAG Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âœ… Ollama tests completed!")
    print("=" * 60)
    
    print("\nğŸ“ å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š")
    print("  1. Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ: ollama serve")
    print("  2. æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½:")
    print(f"     ollama pull {settings.ollama_model}")
    print(f"     ollama pull {settings.ollama_embedding_model}")
    
    return True


if __name__ == "__main__":
    asyncio.run(test_ollama())
